{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dsdl\n",
    "import numpy.random\n",
    "from typing import Callable, List\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "DATASETS = ['a1a', 'mushrooms', 'a6a', 'w1a', 'w5a', 'ionosphere']\n",
    "METHODS = ['SGD', 'AdaSpider', 'Spider', 'SpiderBoost', 'SVRG', 'AdaGrad', 'AdaSVRG']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from src.optimizers.Optimizer import Optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def get_data(dataset_name: str):\n",
    "    \"\"\"\n",
    "    :param dataset_name: Name of the dataset from dsdl module.\n",
    "    :return: (X, y) train and target data.\n",
    "    \"\"\"\n",
    "    ds = dsdl.load(dataset_name)\n",
    "    X, y = ds.get_train()\n",
    "    X = X.toarray()\n",
    "    y = y.reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def build_model(X, y):\n",
    "    \"\"\"\n",
    "    Builds\n",
    "    :param X: shape=(N, D). Train data\n",
    "    :param y: shape=(N, 1). Target data\n",
    "    :return: shape=(N, D+1). Built model for logistic regression.\n",
    "    \"\"\"\n",
    "    return np.c_[np.ones((y.shape[0], 1)), X]\n",
    "\n",
    "def get_initial_weights(tx):\n",
    "    \"\"\"\n",
    "    Returns weights initialized from the uniform distribution [0, 1].\n",
    "    :param tx: shape=(N, D). Build model\n",
    "    :return: shape=(D, 1) Initial weights\n",
    "    \"\"\"\n",
    "    np.random.seed(2022)\n",
    "    return np.ones(shape=(tx.shape[1], 1))  # low=0, high=1,\n",
    "\n",
    "def test_method(method: Optimizer,\n",
    "                initial_weights,\n",
    "                tx,\n",
    "                y,\n",
    "                max_iter: int,\n",
    "                *parameter):\n",
    "        \"\"\"\n",
    "        :param method: Optimization method implementation from src optimizers module.\n",
    "        :param dataset_name: Name of the dataset from dsdl module.\n",
    "        :param max_iter: Number of iterations to test.\n",
    "        :param parameters optional: Dataclass containing parameters used int optimization method.\n",
    "        :return: List of gradients from optimization method.\n",
    "        \"\"\"\n",
    "        gradients, loss = method.optimize(initial_weights, tx, y, max_iter)\n",
    "        return [np.linalg.norm(grad, 2) for grad in gradients], loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from src.optimizers.SGD import SGD\n",
    "from src.optimizers.AdaSpider import AdaSpider\n",
    "from src.optimizers.Spider import Spider\n",
    "from src.optimizers.SpiderBoost import SpiderBoost\n",
    "from src.optimizers.SVRG import SVRG\n",
    "from src.optimizers.AdaGrad import AdaGrad\n",
    "from src.optimizers.AdaSVRG import AdaSVRG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "N_ORACLE_CALLS = 100\n",
    "\n",
    "METHODS = [\n",
    "    SGD(lambda_=0.01, q=N_ORACLE_CALLS),\n",
    "    AdaSpider(q=N_ORACLE_CALLS),\n",
    "    Spider(n_0 = 1, epsilon=0.01, q=N_ORACLE_CALLS),\n",
    "    SpiderBoost(q=N_ORACLE_CALLS),\n",
    "    SVRG(lambda_=0.001, q=N_ORACLE_CALLS),\n",
    "    AdaGrad(lambda_=0.5, epsilon= 0.00001, q=N_ORACLE_CALLS),\n",
    "    AdaSVRG(lambda_=0.1, q=N_ORACLE_CALLS)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset a1a\n",
      "Method SGD\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [54]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m losses_runs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m()\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(N_TESTS):\n\u001B[1;32m---> 25\u001B[0m     gradients, losses \u001B[38;5;241m=\u001B[39m \u001B[43mtest_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mITERATIONS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     gradients_5_runs\u001B[38;5;241m.\u001B[39mappend(gradients)\n\u001B[0;32m     27\u001B[0m gradients_mean \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(gradients_5_runs, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "Input \u001B[1;32mIn [50]\u001B[0m, in \u001B[0;36mtest_method\u001B[1;34m(method, initial_weights, tx, y, max_iter, *parameter)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtest_method\u001B[39m(method: Optimizer,\n\u001B[0;32m     31\u001B[0m                 initial_weights,\n\u001B[0;32m     32\u001B[0m                 tx,\n\u001B[0;32m     33\u001B[0m                 y,\n\u001B[0;32m     34\u001B[0m                 max_iter: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m     35\u001B[0m                 \u001B[38;5;241m*\u001B[39mparameter):\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124;03m        :param method: Optimization method implementation from src optimizers module.\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03m        :param dataset_name: Name of the dataset from dsdl module.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03m        :return: List of gradients from optimization method.\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m        \"\"\"\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m         gradients, loss \u001B[38;5;241m=\u001B[39m method\u001B[38;5;241m.\u001B[39moptimize(initial_weights, tx, y, max_iter)\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(grad, \u001B[38;5;241m2\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m grad \u001B[38;5;129;01min\u001B[39;00m gradients], loss\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1800x1296 with 6 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ITERATIONS = 1000\n",
    "N_TESTS = 1\n",
    "\n",
    "# def plot_data():\n",
    "# Write your code to make 4x4 panel here\n",
    "X_LABEL = \"Stochastic oracle calls\"\n",
    "Y_LABEL = \"||\\u0394f(x)||^2\"\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(25, 18), sharey=False, sharex=False)\n",
    "\n",
    "for i, dataset_name in enumerate(DATASETS):\n",
    "\n",
    "    print(\"Dataset\", dataset_name)\n",
    "    sbplt = ax[i%3, i%2]\n",
    "\n",
    "    X, y = get_data(dataset_name)\n",
    "    tx = build_model(X, y)\n",
    "    initial_weights = get_initial_weights(tx)\n",
    "\n",
    "    for method in METHODS:\n",
    "        print(\"Method\", method.name)\n",
    "        gradients_5_runs = list()\n",
    "        losses_runs = list()\n",
    "        for _ in range(N_TESTS):\n",
    "            gradients, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "            gradients_5_runs.append(gradients)\n",
    "        gradients_mean = np.mean(gradients_5_runs, axis=0)\n",
    "\n",
    "        stddev = np.std(gradients_5_runs, axis=0)\n",
    "        lower = gradients_mean - stddev\n",
    "        upper = gradients_mean + stddev\n",
    "\n",
    "        sbplt.plot(gradients_mean, label=method.name)\n",
    "        sbplt.fill_between(list(range(len(gradients_mean))), lower, upper, alpha=0.25,\n",
    "                           facecolor='red', edgecolor='red')\n",
    "\n",
    "    sbplt.set_xscale('log')\n",
    "    sbplt.set_title(dataset_name)\n",
    "    sbplt.set_xlabel(X_LABEL)\n",
    "    sbplt.set_ylabel(Y_LABEL)\n",
    "    sbplt.legend(loc='lower left')\n",
    "\n",
    "    break  # plot single dataset\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig('tests_logistic_regression.jpg', dpi=300)\n",
    "\n",
    "# plot_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do parameter sweep"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from itertools import product"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset a1a Iterations:  5000\n",
      "Method SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PB\\Documents\\GithubRepos\\EPFL\\ML-reproducibility-challenge\\src\\logistic_regression\\sigmoid.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method AdaSpider\n",
      "Method Spider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PB\\Documents\\GithubRepos\\EPFL\\ML-reproducibility-challenge\\src\\logistic_regression\\log_reg.py:39: RuntimeWarning: overflow encountered in exp\n",
      "  first_component = np.log(1 + np.exp(tx.dot(w)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method SpiderBoost\n",
      "Method SVRG\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [71]\u001B[0m, in \u001B[0;36m<cell line: 27>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m parameters:\n\u001B[0;32m     37\u001B[0m         method\u001B[38;5;241m.\u001B[39mset_params(param)\n\u001B[1;32m---> 38\u001B[0m         _, losses \u001B[38;5;241m=\u001B[39m \u001B[43mtest_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mITERATIONS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m         df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(DF_COLUMNS, [method\u001B[38;5;241m.\u001B[39mname, np\u001B[38;5;241m.\u001B[39msum(losses), \u001B[38;5;28mstr\u001B[39m(param)])), ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Input \u001B[1;32mIn [55]\u001B[0m, in \u001B[0;36mtest_method\u001B[1;34m(method, initial_weights, tx, y, max_iter, *parameter)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtest_method\u001B[39m(method: Optimizer,\n\u001B[0;32m     31\u001B[0m                 initial_weights,\n\u001B[0;32m     32\u001B[0m                 tx,\n\u001B[0;32m     33\u001B[0m                 y,\n\u001B[0;32m     34\u001B[0m                 max_iter: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m     35\u001B[0m                 \u001B[38;5;241m*\u001B[39mparameter):\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124;03m        :param method: Optimization method implementation from src optimizers module.\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03m        :param dataset_name: Name of the dataset from dsdl module.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03m        :return: List of gradients from optimization method.\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m        \"\"\"\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m         gradients, loss \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(grad, \u001B[38;5;241m2\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m grad \u001B[38;5;129;01min\u001B[39;00m gradients], loss\n",
      "File \u001B[1;32m~\\Documents\\GithubRepos\\EPFL\\ML-reproducibility-challenge\\src\\optimizers\\SVRG.py:40\u001B[0m, in \u001B[0;36mSVRG.optimize\u001B[1;34m(self, w_0, tx, y, max_iter)\u001B[0m\n\u001B[0;32m     37\u001B[0m     v \u001B[38;5;241m=\u001B[39m log_reg_gradient(y, tx, w[k])\n\u001B[0;32m     39\u001B[0m i_k \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(y)))\n\u001B[1;32m---> 40\u001B[0m grad \u001B[38;5;241m=\u001B[39m stochastic_gradient(y, tx, w[k], [i_k]) \u001B[38;5;241m-\u001B[39m \u001B[43mstochastic_gradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mi_k\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m v\n\u001B[0;32m     42\u001B[0m next_w \u001B[38;5;241m=\u001B[39m w[k] \u001B[38;5;241m-\u001B[39m step_size \u001B[38;5;241m*\u001B[39m grad\n\u001B[0;32m     44\u001B[0m w\u001B[38;5;241m.\u001B[39mappend(next_w)\n",
      "File \u001B[1;32m~\\Documents\\GithubRepos\\EPFL\\ML-reproducibility-challenge\\src\\logistic_regression\\stochastic_gradient.py:10\u001B[0m, in \u001B[0;36mstochastic_gradient\u001B[1;34m(y, tx, w, sample_indices)\u001B[0m\n\u001B[0;32m      8\u001B[0m y_sgd \u001B[38;5;241m=\u001B[39m y[sample_indices]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      9\u001B[0m x_sgd \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(tx[sample_indices, :])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlog_reg_gradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_sgd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_sgd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GithubRepos\\EPFL\\ML-reproducibility-challenge\\src\\logistic_regression\\log_reg.py:22\u001B[0m, in \u001B[0;36mlog_reg_gradient\u001B[1;34m(y, tx, w)\u001B[0m\n\u001B[0;32m     20\u001B[0m lamb \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[0;32m     21\u001B[0m pred \u001B[38;5;241m=\u001B[39m sigmoid(tx\u001B[38;5;241m.\u001B[39mdot(w))\n\u001B[1;32m---> 22\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[43mtx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m y\u001B[38;5;241m.\u001B[39msize) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m lamb \u001B[38;5;241m*\u001B[39m non_convex(w)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grad\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "REDUCTION_STEP = 100\n",
    "\n",
    "METHODS = [\n",
    "    SGD(lambda_=0.01, q=REDUCTION_STEP),\n",
    "    AdaSpider(q=REDUCTION_STEP),\n",
    "    Spider(n_0 = 1, epsilon=0.01, q=REDUCTION_STEP),\n",
    "    SpiderBoost(q=REDUCTION_STEP),\n",
    "    SVRG(lambda_=0.001, q=REDUCTION_STEP),\n",
    "    AdaGrad(lambda_=0.5, epsilon= 0.00001, q=REDUCTION_STEP),\n",
    "    AdaSVRG(lambda_=0.1, q=REDUCTION_STEP)\n",
    "]\n",
    "\n",
    "DF_COLUMNS = ['method_name', 'loss', 'parameters']\n",
    "df = pd.DataFrame(columns=DF_COLUMNS)\n",
    "\n",
    "dataset_name = 'a1a'\n",
    "ITERATIONS = 5000\n",
    "print(\"Dataset\", dataset_name, \"Iterations: \", ITERATIONS)\n",
    "\n",
    "powers = np.array([range(-5, 0)], dtype=float)\n",
    "parameters = 10**powers.flatten()\n",
    "\n",
    "X, y = get_data(dataset_name)\n",
    "tx = build_model(X, y)\n",
    "initial_weights = get_initial_weights(tx)\n",
    "\n",
    "for method in METHODS:\n",
    "    print(\"Method\", method.name)\n",
    "    if method.n_params_to_tune == 2:\n",
    "        for param in list(product(parameters, parameters)):\n",
    "            method.set_params(param[0], param[1])\n",
    "            _, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "            df = df.append(dict(zip(DF_COLUMNS, [method.name, np.sum(losses), str(param)])), ignore_index=True)\n",
    "\n",
    "    elif method.n_params_to_tune == 1:\n",
    "        for param in parameters:\n",
    "            method.set_params(param)\n",
    "            _, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "            df = df.append(dict(zip(DF_COLUMNS, [method.name, np.sum(losses), str(param)])), ignore_index=True)\n",
    "    else:\n",
    "        _, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "        df = df.append(dict(zip(DF_COLUMNS, [method.name, np.sum(losses), str(param)])), ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "                     loss        parameters\nmethod_name                                \nAdaGrad     -2.064983e+06  (0.0001, 0.0001)\nAdaSVRG     -8.536473e+02            0.0001\nAdaSpider    6.328202e+03               0.1\nSGD         -2.830776e+04            0.0001\nSVRG        -1.728072e+04            0.0001\nSpider      -1.862576e+08  (0.0001, 0.0001)\nSpiderBoost -4.883895e+02        (0.1, 0.1)",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>parameters</th>\n    </tr>\n    <tr>\n      <th>method_name</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AdaGrad</th>\n      <td>-2.064983e+06</td>\n      <td>(0.0001, 0.0001)</td>\n    </tr>\n    <tr>\n      <th>AdaSVRG</th>\n      <td>-8.536473e+02</td>\n      <td>0.0001</td>\n    </tr>\n    <tr>\n      <th>AdaSpider</th>\n      <td>6.328202e+03</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>SGD</th>\n      <td>-2.830776e+04</td>\n      <td>0.0001</td>\n    </tr>\n    <tr>\n      <th>SVRG</th>\n      <td>-1.728072e+04</td>\n      <td>0.0001</td>\n    </tr>\n    <tr>\n      <th>Spider</th>\n      <td>-1.862576e+08</td>\n      <td>(0.0001, 0.0001)</td>\n    </tr>\n    <tr>\n      <th>SpiderBoost</th>\n      <td>-4.883895e+02</td>\n      <td>(0.1, 0.1)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('method_name').min()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# METHODS = ['AdaSpider', 'Spider']\n",
    "# ITERATIONS = 100\n",
    "#\n",
    "# def plot_data():\n",
    "#     # Write your code to make 4x4 panel here\n",
    "#     X_LABEL = \"Stochastic oracle calls\"\n",
    "#     Y_LABEL = \"||\\u0394f(x)||^2\"\n",
    "#\n",
    "#     fig, ax = plt.subplots(3,2,figsize=(16,16), sharey=False, sharex=False)\n",
    "#\n",
    "#     for i, dataset_name in enumerate(DATASETS):\n",
    "#         sbplt = ax[i%3, i%2]\n",
    "#         print(dataset_name)\n",
    "#         for method in METHODS:\n",
    "#             if method == \"Spider\":\n",
    "#                 spider_params = SpiderParam(100, 5, 0.05)\n",
    "#                 gradients = test_method(Spider, dataset_name, ITERATIONS, spider_params)\n",
    "#             else:\n",
    "#                 gradients = test_method(ADASpider, dataset_name, ITERATIONS)\n",
    "#             gradients = [np.linalg.norm(grad, 2) for grad in gradients]\n",
    "#             sbplt.plot(gradients, label=method)\n",
    "#\n",
    "#         sbplt.set_xscale('log')\n",
    "#         sbplt.set_title(dataset_name)\n",
    "#         sbplt.set_xlabel(X_LABEL)\n",
    "#         sbplt.set_ylabel(Y_LABEL)\n",
    "#         sbplt.legend(loc='lower left')\n",
    "#\n",
    "#         break  # plot single dataset\n",
    "#\n",
    "#     fig.tight_layout(pad=2.0)\n",
    "#     # fig.savefig('tests_logistic_regression.jpg', dpi=150)\n",
    "#\n",
    "# plot_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}