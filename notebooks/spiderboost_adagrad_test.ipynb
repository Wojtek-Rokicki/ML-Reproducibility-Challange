{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "from misc.helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w, type='mse'):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "        type: string that can take value 'mse' or 'mae'.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # Compute loss by MSE\n",
    "    error = y - np.dot(tx, w)\n",
    "    N = len(tx)\n",
    "    if type == 'mse':\n",
    "        loss = 1/(2*N)*np.sum(np.square(error))\n",
    "    elif type == 'mae':\n",
    "        loss = 1/N*np.sum(np.abs(error))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # Compute gradient vector\n",
    "    error = y - np.dot(tx, w)\n",
    "    N = len(tx)\n",
    "    gradient_w0 = -1/N*np.sum(error)\n",
    "    gradient_w1 = -1/N*np.sum(error*tx[:,1])\n",
    "\n",
    "    gradient = np.array([gradient_w0, gradient_w1]).T\n",
    "\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    N = len(tx)\n",
    "    error = y - np.dot(tx, w)\n",
    "    stoch_gradient = -1/N*np.dot(tx.T, error)\n",
    "\n",
    "    return stoch_gradient\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1, compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaGrad(w_0, tx, y, max_iter, lambda_, epsilon = 1e-8):\n",
    "        '''Algoritm for adaptive gradient optimization.\n",
    "\n",
    "        Adapts learing parameter - smaller rate for frequent features (well-suited for sparse data).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w_0 : ndarray of shape (D, 1)\n",
    "            Initial weights of the model\n",
    "        tx : ndarray of shape (N, D)\n",
    "            Array of input features\n",
    "        y : ndarray of shape (N, 1)\n",
    "            Array of output\n",
    "        max_iter : int\n",
    "            Maximum number of iterations\n",
    "        Returns\n",
    "        -------\n",
    "        grads : ndarray of shape (max_iter, D)\n",
    "            Array of gradient estimators in each step of the algorithm.\n",
    "        '''\n",
    "        D = len(w_0)\n",
    "        G_t = np.zeros((D, D))\n",
    "\n",
    "        # Outputs\n",
    "        grads = []\n",
    "        w = [w_0]\n",
    "        losses = []\n",
    "\n",
    "        for t in range(max_iter):\n",
    "            g_t = compute_gradient(y, tx, w[t])\n",
    "            G_t += g_t**2\n",
    "            v_k = np.diag(lambda_ / np.sqrt(G_t + epsilon)) @ g_t\n",
    "            w_next = w[t] - v_k\n",
    "            w.append(w_next)\n",
    "            grads.append(v_k)\n",
    "\n",
    "            loss = compute_loss(y, tx, w[t])\n",
    "            losses.append(loss)\n",
    "\n",
    "            print(\"AG iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=t, ti=max_iter - 1, l=loss, w0=w[t][0], w1=w[t][1]))\n",
    "\n",
    "        return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG iter. 0/49: loss=2792.2367127591674, w0=0, w1=0\n",
      "AG iter. 1/49: loss=1456.7640240305902, w0=19.99999999971552, w1=19.99999999971552\n",
      "AG iter. 2/49: loss=1387.694382451022, w0=21.526497503422412, w1=21.526497503422412\n",
      "AG iter. 3/49: loss=1377.8409098229004, w0=21.753044884084776, w1=21.753044884084776\n",
      "AG iter. 4/49: loss=1375.6019571486906, w0=21.804853620553107, w1=21.804853620553107\n",
      "AG iter. 5/49: loss=1374.9427088473406, w0=21.820132159322355, w1=21.820132159322355\n",
      "AG iter. 6/49: loss=1374.7130239259768, w0=21.82545781051292, w1=21.82545781051292\n",
      "AG iter. 7/49: loss=1374.622839817151, w0=21.827549248284324, w1=21.827549248284324\n",
      "AG iter. 8/49: loss=1374.5840937154567, w0=21.82844786203259, w1=21.82844786203259\n",
      "AG iter. 9/49: loss=1374.5662282819485, w0=21.828862216311954, w1=21.828862216311954\n",
      "AG iter. 10/49: loss=1374.557505946151, w0=21.829064516964355, w1=21.829064516964355\n",
      "AG iter. 11/49: loss=1374.5530408523907, w0=21.82916807841965, w1=21.82916807841965\n",
      "AG iter. 12/49: loss=1374.5506617867998, w0=21.829223257642113, w1=21.829223257642113\n",
      "AG iter. 13/49: loss=1374.5493499309605, w0=21.829253684432032, w1=21.829253684432032\n",
      "AG iter. 14/49: loss=1374.5486046615165, w0=21.829270970006572, w1=21.829270970006572\n",
      "AG iter. 15/49: loss=1374.5481700396124, w0=21.8292810505139, w1=21.8292810505139\n",
      "AG iter. 16/49: loss=1374.5479106275477, w0=21.829287067252054, w1=21.829287067252054\n",
      "AG iter. 17/49: loss=1374.5477525474128, w0=21.829290733723752, w1=21.829290733723752\n",
      "AG iter. 18/49: loss=1374.5476544014473, w0=21.82929301009741, w1=21.82929301009741\n",
      "AG iter. 19/49: loss=1374.547592427256, w0=21.82929444751186, w1=21.82929444751186\n",
      "AG iter. 20/49: loss=1374.5475526864207, w0=21.829295369251252, w1=21.829295369251252\n",
      "AG iter. 21/49: loss=1374.5475268409687, w0=21.829295968704486, w1=21.829295968704486\n",
      "AG iter. 22/49: loss=1374.547509813159, w0=21.82929636364344, w1=21.82929636364344\n",
      "AG iter. 23/49: loss=1374.547498459699, w0=21.829296626972884, w1=21.829296626972884\n",
      "AG iter. 24/49: loss=1374.547490805307, w0=21.829296804507035, w1=21.829296804507035\n",
      "AG iter. 25/49: loss=1374.5474855913485, w0=21.829296925438346, w1=21.829296925438346\n",
      "AG iter. 26/49: loss=1374.5474820054599, w0=21.82929700860859, w1=21.82929700860859\n",
      "AG iter. 27/49: loss=1374.5474795170162, w0=21.82929706632496, w1=21.82929706632496\n",
      "AG iter. 28/49: loss=1374.5474777755421, w0=21.829297106716307, w1=21.829297106716307\n",
      "AG iter. 29/49: loss=1374.5474765471272, w0=21.829297135207863, w1=21.829297135207863\n",
      "AG iter. 30/49: loss=1374.5474756741326, w0=21.8292971554559, w1=21.8292971554559\n",
      "AG iter. 31/49: loss=1374.5474750493395, w0=21.829297169947196, w1=21.829297169947196\n",
      "AG iter. 32/49: loss=1374.5474745991949, w0=21.829297180387748, w1=21.829297180387748\n",
      "AG iter. 33/49: loss=1374.547474272826, w0=21.829297187957465, w1=21.829297187957465\n",
      "AG iter. 34/49: loss=1374.5474740347775, w0=21.829297193478713, w1=21.829297193478713\n",
      "AG iter. 35/49: loss=1374.5474738601565, w0=21.829297197528827, w1=21.829297197528827\n",
      "AG iter. 36/49: loss=1374.5474737313668, w0=21.829297200515942, w1=21.829297200515942\n",
      "AG iter. 37/49: loss=1374.5474736358874, w0=21.82929720273047, w1=21.82929720273047\n",
      "AG iter. 38/49: loss=1374.5474735647526, w0=21.829297204380357, w1=21.829297204380357\n",
      "AG iter. 39/49: loss=1374.5474735115044, w0=21.829297205615383, w1=21.829297205615383\n",
      "AG iter. 40/49: loss=1374.5474734714649, w0=21.829297206544048, w1=21.829297206544048\n",
      "AG iter. 41/49: loss=1374.547473441227, w0=21.82929720724538, w1=21.82929720724538\n",
      "AG iter. 42/49: loss=1374.5474734182962, w0=21.82929720777723, w1=21.82929720777723\n",
      "AG iter. 43/49: loss=1374.5474734008374, w0=21.82929720818217, w1=21.82929720818217\n",
      "AG iter. 44/49: loss=1374.5474733874935, w0=21.82929720849166, w1=21.82929720849166\n",
      "AG iter. 45/49: loss=1374.5474733772573, w0=21.829297208729077, w1=21.829297208729077\n",
      "AG iter. 46/49: loss=1374.5474733693768, w0=21.829297208911854, w1=21.829297208911854\n",
      "AG iter. 47/49: loss=1374.5474733632893, w0=21.829297209053045, w1=21.829297209053045\n",
      "AG iter. 48/49: loss=1374.5474733585713, w0=21.829297209162476, w1=21.829297209162476\n",
      "AG iter. 49/49: loss=1374.5474733549026, w0=21.82929720924756, w1=21.82929720924756\n",
      "AG: execution time=0.023 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start AG.\n",
    "start_time = datetime.datetime.now()\n",
    "ag_losses, ag_ws = AdaGrad(w_initial, tx, y, max_iters, 1e1)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"AG: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "# Near optimum: loss=3.0832601417144585, w0=73.10945680053558, w1=11.967378197209419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpiderBoost(w_0, tx, y, max_iter):\n",
    "        \"\"\"Algorithm for gradient optimization, which estimates gradients and reduces their iterative variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w_0 : ndarray of shape (D, 1)\n",
    "            Initial weights of the model\n",
    "        tx : ndarray of shape (N, D)\n",
    "            Array of input features\n",
    "        y : ndarray of shape (N, 1)\n",
    "            Array of output\n",
    "        max_iter : int\n",
    "            Maximum number of iterations\n",
    "        Returns\n",
    "        -------\n",
    "        grads : ndarray of shape (max_iter, D)\n",
    "            Array of gradient estimators in each step of the algorithm.\n",
    "        \"\"\"\n",
    "        # Intrinsic parameters initialization\n",
    "        N = len(tx)\n",
    "        n = len(y)\n",
    "        lipshitz_const = np.linalg.norm(tx, ord='fro')**2\n",
    "\n",
    "        # Outputs\n",
    "        grads = []\n",
    "        w = [w_0]\n",
    "        losses = []\n",
    "\n",
    "        # Algorithm\n",
    "        for t in range(max_iter):\n",
    "            if t % n == 0:\n",
    "                v_k = compute_gradient(y, tx, w[t]) # logistic cost function full gradient\n",
    "            else:\n",
    "                i = np.random.choice(np.arange(1, n))\n",
    "                v_k = partial_sum = compute_stoch_gradient(y[i], tx[i], w[t]) - compute_stoch_gradient(y[i], tx[i], w[t-1]) + v_k\n",
    "            w_next = w[t] - 1/(2*lipshitz_const)*v_k\n",
    "            w.append(w_next)\n",
    "            grads.append(v_k)\n",
    "\n",
    "            loss = compute_loss(y, tx, w[t])\n",
    "            losses.append(loss)\n",
    "\n",
    "            print(\"SB iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=t, ti=max_iter - 1, l=loss, w0=w[t][0], w1=w[t][1]))\n",
    "\n",
    "        return losses, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB iter. 0/49: loss=2792.2367127591674, w0=0, w1=0\n",
      "SB iter. 1/49: loss=2792.097871953454, w0=0.0018323480500526278, w1=0.00033699281087472396\n",
      "SB iter. 2/49: loss=2791.9590363295856, w0=0.0036646729777044798, w1=0.0006739844248472487\n",
      "SB iter. 3/49: loss=2791.8202057745266, w0=0.005496975559131376, w1=0.0010109789980727712\n",
      "SB iter. 4/49: loss=2791.6813807190574, w0=0.007329252960889827, w1=0.0013479599672669162\n",
      "SB iter. 5/49: loss=2791.542560435041, w0=0.009161510201058645, w1=0.001684954060197257\n",
      "SB iter. 6/49: loss=2791.40374522717, w0=0.010993745041640435, w1=0.0020219508311733227\n",
      "SB iter. 7/49: loss=2791.2649362745337, w0=0.012825950381452754, w1=0.0023589013960855754\n",
      "SB iter. 8/49: loss=2791.1261318436464, w0=0.014658137595163353, w1=0.00269587251681817\n",
      "SB iter. 9/49: loss=2790.9873328850827, w0=0.01649029979548199, w1=0.00303283110126795\n",
      "SB iter. 10/49: loss=2790.8485390777146, w0=0.018322439075520773, w1=0.0033697895863941947\n",
      "SB iter. 11/49: loss=2790.7097510597487, w0=0.020154551432608112, w1=0.003706722367973382\n",
      "SB iter. 12/49: loss=2790.570967896212, w0=0.021986642989199345, w1=0.004043665525494239\n",
      "SB iter. 13/49: loss=2790.4321897434506, w0=0.02381871260147162, w1=0.0043806136682794745\n",
      "SB iter. 14/49: loss=2790.293416693537, w0=0.025650759622550437, w1=0.004717563472083759\n",
      "SB iter. 15/49: loss=2790.1546485047006, w0=0.02748278579121506, w1=0.005054523416119779\n",
      "SB iter. 16/49: loss=2790.015886096158, w0=0.02931478508647093, w1=0.0053914580110900525\n",
      "SB iter. 17/49: loss=2789.8771289308693, w0=0.031146760837056465, w1=0.005728389002094909\n",
      "SB iter. 18/49: loss=2789.7383770275683, w0=0.03297871291821666, w1=0.006065315667221589\n",
      "SB iter. 19/49: loss=2789.599630504733, w0=0.034810640559427695, w1=0.006402233392499381\n",
      "SB iter. 20/49: loss=2789.460889074364, w0=0.03664254567681637, w1=0.006739153124815765\n",
      "SB iter. 21/49: loss=2789.3221524155647, w0=0.038474430618847644, w1=0.007076085903968938\n",
      "SB iter. 22/49: loss=2789.183421612706, w0=0.04030628824700755, w1=0.007412990048520624\n",
      "SB iter. 23/49: loss=2789.044695656987, w0=0.04213812512098215, w1=0.007749904759330164\n",
      "SB iter. 24/49: loss=2788.9059748751492, w0=0.04396993891024914, w1=0.008086818446499566\n",
      "SB iter. 25/49: loss=2788.76725820392, w0=0.045801738538092905, w1=0.008423761510385012\n",
      "SB iter. 26/49: loss=2788.6285475133454, w0=0.047633510144822644, w1=0.008760670486730184\n",
      "SB iter. 27/49: loss=2788.489841236949, w0=0.049465264561409296, w1=0.009097602758193702\n",
      "SB iter. 28/49: loss=2788.351140023622, w0=0.05129699665356777, w1=0.009434538064280244\n",
      "SB iter. 29/49: loss=2788.212444465945, w0=0.05312870259844339, w1=0.009771453189593854\n",
      "SB iter. 30/49: loss=2788.073753667429, w0=0.05496038845758764, w1=0.010108381719545124\n",
      "SB iter. 31/49: loss=2787.935067741101, w0=0.05679205336923309, w1=0.010445319941227927\n",
      "SB iter. 32/49: loss=2787.796386855464, w0=0.05862369611088443, w1=0.010782261983519725\n",
      "SB iter. 33/49: loss=2787.6577118203245, w0=0.06045531156247625, w1=0.011119175551631244\n",
      "SB iter. 34/49: loss=2787.5190412797683, w0=0.062286909102196315, w1=0.011456110314641893\n",
      "SB iter. 35/49: loss=2787.3803772614583, w0=0.06411847574095798, w1=0.011792986338035329\n",
      "SB iter. 36/49: loss=2787.2417181770547, w0=0.06595002097242335, w1=0.01212986992237925\n",
      "SB iter. 37/49: loss=2787.1030638010516, w0=0.06778154651146996, w1=0.01246676847998811\n",
      "SB iter. 38/49: loss=2786.9644151538014, w0=0.06961304546367147, w1=0.012803643723929899\n",
      "SB iter. 39/49: loss=2786.8257712020486, w0=0.07144452482450579, w1=0.01314053433212034\n",
      "SB iter. 40/49: loss=2786.6871335314704, w0=0.07327597452606446, w1=0.013477377288464867\n",
      "SB iter. 41/49: loss=2786.5485004248026, w0=0.07510740571399369, w1=0.013814239501329333\n",
      "SB iter. 42/49: loss=2786.4098738558528, w0=0.07693880591865818, w1=0.014151042184518528\n",
      "SB iter. 43/49: loss=2786.27125333213, w0=0.07877017773329953, w1=0.01448780779561909\n",
      "SB iter. 44/49: loss=2786.1326377539167, w0=0.08060152804919056, w1=0.014824580522957393\n",
      "SB iter. 45/49: loss=2785.994027326224, w0=0.08243285542331243, w1=0.015161352978404395\n",
      "SB iter. 46/49: loss=2785.8554211800824, w0=0.08426416683956922, w1=0.015498151718063131\n",
      "SB iter. 47/49: loss=2785.71681950786, w0=0.08609546051285015, w1=0.015834972155721286\n",
      "SB iter. 48/49: loss=2785.578222642673, w0=0.08792673371680851, w1=0.016171804367865656\n",
      "SB iter. 49/49: loss=2785.439630567481, w0=0.08975798658142756, w1=0.016508648905996143\n",
      "SB: execution time=0.032 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SB.\n",
    "start_time = datetime.datetime.now()\n",
    "sb_losses, sb_ws = SpiderBoost(w_initial, tx, y, max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SB: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bddca6f205aebd86508050b416256222382b98c4aa05f61f33afbd3d1facfd66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
