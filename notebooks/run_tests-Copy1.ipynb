{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import dsdl\n",
    "import numpy.random\n",
    "from typing import Callable, List\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/fkunstner/dataset-downloader.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS = ['a1a', 'mushrooms', 'a6a', 'w1a', 'w5a', 'ionosphere']\n",
    "METHODS = ['SGD', 'AdaSpider', 'Spider', 'SpiderBoost', 'SVRG', 'AdaGrad', 'AdaSVRG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = \"/root/ML-Reproducibility-Challange/\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for dataset_name in DATASETS:\n",
    "#     X, y = get_data(dataset_name)\n",
    "#     pd.DataFrame(np.c_[X, y]).to_csv(f\"{dataset_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.optimizers.Optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset_name: str):\n",
    "    \"\"\"\n",
    "    :param dataset_name: Name of the dataset from dsdl module.\n",
    "    :return: (X, y) train and target data.\n",
    "    \"\"\"\n",
    "    ds = dsdl.load(dataset_name)\n",
    "    X, y = ds.get_train()\n",
    "    X = X.toarray()\n",
    "    y = y.reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def build_model(X, y):\n",
    "    \"\"\"\n",
    "    Joins weights\n",
    "    :param X: shape=(N, D). Train data\n",
    "    :param y: shape=(N, 1). Target data\n",
    "    :return: shape=(N, D+1). Built model for logistic regression.\n",
    "    \"\"\"\n",
    "    return np.c_[np.zeros((y.shape[0], 1)), X]\n",
    "\n",
    "def get_initial_weights(tx):\n",
    "    \"\"\"\n",
    "    Returns weights initialized from the uniform distribution [0, 1].\n",
    "    :param tx: shape=(N, D). Build model\n",
    "    :return: shape=(D, 1) Initial weights\n",
    "    \"\"\"\n",
    "    np.random.seed(2022)\n",
    "    return np.zeros(shape=(tx.shape[1], 1))\n",
    "\n",
    "def test_method(method: Optimizer,\n",
    "                initial_weights,\n",
    "                tx,\n",
    "                y,\n",
    "                max_iter: int,\n",
    "                *parameter):\n",
    "        \"\"\"\n",
    "        :param method: Optimization method implementation from src optimizers module.\n",
    "        :param dataset_name: Name of the dataset from dsdl module.\n",
    "        :param max_iter: Number of iterations to test.\n",
    "        :param parameters optional: Dataclass containing parameters used int optimization method.\n",
    "        :return: List of gradients from optimization method.\n",
    "        \"\"\"\n",
    "        gradients, loss = method.optimize(initial_weights, tx, y, max_iter)\n",
    "        return [np.linalg.norm(grad, 2) for grad in gradients], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.optimizers.SGD import SGD\n",
    "from src.optimizers.AdaSpider import AdaSpider\n",
    "from src.optimizers.Spider import Spider\n",
    "from src.optimizers.SpiderBoost import SpiderBoost\n",
    "from src.optimizers.SVRG import SVRG\n",
    "from src.optimizers.AdaGrad import AdaGrad\n",
    "from src.optimizers.AdaSVRG import AdaSVRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_ORACLE_CALLS = 1300\n",
    "\n",
    "METHODS = [\n",
    "    # SGD(lambda_=0.0001, q=N_ORACLE_CALLS),\n",
    "    # AdaSpider(q=N_ORACLE_CALLS),\n",
    "    # Spider(n_0 = 0.0001, epsilon=0.0001, q=N_ORACLE_CALLS),\n",
    "    # SpiderBoost(q=N_ORACLE_CALLS),\n",
    "    SVRG(lambda_=0.0001, q=N_ORACLE_CALLS)\n",
    "    # AdaGrad(lambda_=0.0001, epsilon= 0.0001, q=N_ORACLE_CALLS),\n",
    "    # AdaSVRG(lambda_=0.01, q=N_ORACLE_CALLS)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run 2 # 20 mins on 8 cpu avs ml.c5.2xlarge\n",
    "# run 1 # 1 hour later on the 4 cpu machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset a1a\n",
      "Method SVRG\n",
      "Full grad, iter: 0\n",
      "Full grad, iter: 1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ML-Reproducibility-Challange/src/logistic_regression/sigmoid.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full grad, iter: 2600\n",
      "Full grad, iter: 3900\n",
      "Full grad, iter: 5200\n",
      "Full grad, iter: 6500\n",
      "Full grad, iter: 7800\n",
      "Full grad, iter: 9100\n",
      "Full grad, iter: 10400\n",
      "Full grad, iter: 11700\n",
      "Full grad, iter: 13000\n",
      "Full grad, iter: 14300\n",
      "Full grad, iter: 15600\n",
      "Full grad, iter: 16900\n",
      "Full grad, iter: 18200\n",
      "Full grad, iter: 19500\n",
      "Full grad, iter: 20800\n",
      "Full grad, iter: 22100\n",
      "Full grad, iter: 23400\n",
      "Full grad, iter: 24700\n",
      "Full grad, iter: 26000\n",
      "Full grad, iter: 27300\n",
      "Full grad, iter: 28600\n",
      "Full grad, iter: 29900\n",
      "Full grad, iter: 31200\n",
      "Full grad, iter: 32500\n",
      "Full grad, iter: 33800\n",
      "Full grad, iter: 35100\n",
      "Full grad, iter: 36400\n",
      "Full grad, iter: 37700\n",
      "Full grad, iter: 39000\n",
      "Full grad, iter: 40300\n",
      "Full grad, iter: 41600\n",
      "Full grad, iter: 42900\n",
      "Full grad, iter: 44200\n",
      "Full grad, iter: 45500\n",
      "Full grad, iter: 46800\n",
      "Full grad, iter: 48100\n",
      "Full grad, iter: 49400\n",
      "Full grad, iter: 50700\n",
      "Full grad, iter: 52000\n",
      "Full grad, iter: 53300\n",
      "Full grad, iter: 54600\n",
      "Full grad, iter: 55900\n",
      "Full grad, iter: 57200\n",
      "Full grad, iter: 58500\n",
      "Full grad, iter: 59800\n",
      "Full grad, iter: 61100\n",
      "Full grad, iter: 62400\n",
      "Full grad, iter: 63700\n",
      "Full grad, iter: 65000\n",
      "Full grad, iter: 66300\n",
      "Full grad, iter: 67600\n",
      "Full grad, iter: 68900\n",
      "Full grad, iter: 70200\n",
      "Full grad, iter: 71500\n",
      "Full grad, iter: 72800\n",
      "Full grad, iter: 74100\n",
      "Full grad, iter: 75400\n",
      "Full grad, iter: 76700\n",
      "Full grad, iter: 78000\n",
      "Full grad, iter: 79300\n",
      "Full grad, iter: 80600\n",
      "Full grad, iter: 81900\n",
      "Full grad, iter: 83200\n",
      "Full grad, iter: 84500\n",
      "Full grad, iter: 85800\n",
      "Full grad, iter: 87100\n",
      "Full grad, iter: 88400\n",
      "Full grad, iter: 89700\n",
      "Full grad, iter: 91000\n",
      "Full grad, iter: 92300\n",
      "Full grad, iter: 93600\n",
      "Full grad, iter: 94900\n",
      "Full grad, iter: 96200\n",
      "Full grad, iter: 97500\n",
      "Full grad, iter: 98800\n",
      "Full grad, iter: 0\n",
      "Full grad, iter: 1300\n",
      "Full grad, iter: 2600\n",
      "Full grad, iter: 3900\n",
      "Full grad, iter: 5200\n",
      "Full grad, iter: 6500\n",
      "Full grad, iter: 7800\n",
      "Full grad, iter: 9100\n",
      "Full grad, iter: 10400\n",
      "Full grad, iter: 11700\n",
      "Full grad, iter: 13000\n",
      "Full grad, iter: 14300\n",
      "Full grad, iter: 15600\n",
      "Full grad, iter: 16900\n",
      "Full grad, iter: 18200\n",
      "Full grad, iter: 19500\n",
      "Full grad, iter: 20800\n",
      "Full grad, iter: 22100\n",
      "Full grad, iter: 23400\n",
      "Full grad, iter: 24700\n",
      "Full grad, iter: 26000\n",
      "Full grad, iter: 27300\n",
      "Full grad, iter: 28600\n",
      "Full grad, iter: 29900\n",
      "Full grad, iter: 31200\n",
      "Full grad, iter: 32500\n",
      "Full grad, iter: 33800\n",
      "Full grad, iter: 35100\n",
      "Full grad, iter: 36400\n",
      "Full grad, iter: 37700\n",
      "Full grad, iter: 39000\n",
      "Full grad, iter: 40300\n",
      "Full grad, iter: 41600\n",
      "Full grad, iter: 42900\n",
      "Full grad, iter: 44200\n",
      "Full grad, iter: 45500\n",
      "Full grad, iter: 46800\n",
      "Full grad, iter: 48100\n",
      "Full grad, iter: 49400\n",
      "Full grad, iter: 50700\n",
      "Full grad, iter: 52000\n",
      "Full grad, iter: 53300\n",
      "Full grad, iter: 54600\n",
      "Full grad, iter: 55900\n",
      "Full grad, iter: 57200\n",
      "Full grad, iter: 58500\n",
      "Full grad, iter: 59800\n",
      "Full grad, iter: 61100\n",
      "Full grad, iter: 62400\n",
      "Full grad, iter: 63700\n",
      "Full grad, iter: 65000\n",
      "Full grad, iter: 66300\n",
      "Full grad, iter: 67600\n",
      "Full grad, iter: 68900\n",
      "Full grad, iter: 70200\n",
      "Full grad, iter: 71500\n",
      "Full grad, iter: 72800\n",
      "Full grad, iter: 74100\n",
      "Full grad, iter: 75400\n",
      "Full grad, iter: 76700\n",
      "Full grad, iter: 78000\n",
      "Full grad, iter: 79300\n",
      "Full grad, iter: 80600\n",
      "Full grad, iter: 81900\n",
      "Full grad, iter: 83200\n",
      "Full grad, iter: 84500\n",
      "Full grad, iter: 85800\n",
      "Full grad, iter: 87100\n",
      "Full grad, iter: 88400\n",
      "Full grad, iter: 89700\n",
      "Full grad, iter: 91000\n",
      "Full grad, iter: 92300\n",
      "Full grad, iter: 93600\n",
      "Full grad, iter: 94900\n",
      "Full grad, iter: 96200\n",
      "Full grad, iter: 97500\n",
      "Full grad, iter: 98800\n",
      "Full grad, iter: 0\n",
      "Full grad, iter: 1300\n",
      "Full grad, iter: 2600\n",
      "Full grad, iter: 3900\n",
      "Full grad, iter: 5200\n",
      "Full grad, iter: 6500\n",
      "Full grad, iter: 7800\n",
      "Full grad, iter: 9100\n",
      "Full grad, iter: 10400\n",
      "Full grad, iter: 11700\n",
      "Full grad, iter: 13000\n",
      "Full grad, iter: 14300\n",
      "Full grad, iter: 15600\n",
      "Full grad, iter: 16900\n",
      "Full grad, iter: 18200\n",
      "Full grad, iter: 19500\n",
      "Full grad, iter: 20800\n",
      "Full grad, iter: 22100\n",
      "Full grad, iter: 23400\n",
      "Full grad, iter: 24700\n",
      "Full grad, iter: 26000\n",
      "Full grad, iter: 27300\n",
      "Full grad, iter: 28600\n",
      "Full grad, iter: 29900\n",
      "Full grad, iter: 31200\n",
      "Full grad, iter: 32500\n",
      "Full grad, iter: 33800\n",
      "Full grad, iter: 35100\n",
      "Full grad, iter: 36400\n",
      "Full grad, iter: 37700\n",
      "Full grad, iter: 39000\n",
      "Full grad, iter: 40300\n",
      "Full grad, iter: 41600\n",
      "Full grad, iter: 42900\n",
      "Full grad, iter: 44200\n",
      "Full grad, iter: 45500\n",
      "Full grad, iter: 46800\n",
      "Full grad, iter: 48100\n",
      "Full grad, iter: 49400\n",
      "Full grad, iter: 50700\n",
      "Full grad, iter: 52000\n",
      "Full grad, iter: 53300\n",
      "Full grad, iter: 54600\n",
      "Full grad, iter: 55900\n",
      "Full grad, iter: 57200\n",
      "Full grad, iter: 58500\n",
      "Full grad, iter: 59800\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 100000\n",
    "N_RUNS = 5\n",
    "\n",
    "all_gradients = []\n",
    "\n",
    "datasets_data = {}\n",
    "for i, dataset_name in enumerate(DATASETS):\n",
    "\n",
    "    print(\"Dataset\", dataset_name)\n",
    "\n",
    "    X, y = get_data(dataset_name)\n",
    "    tx = build_model(X, y)\n",
    "    initial_weights = get_initial_weights(tx)\n",
    "\n",
    "    methods_data = {}\n",
    "    for method in METHODS:\n",
    "        print(\"Method\", method.name)\n",
    "\n",
    "        gradients_5_runs = list()\n",
    "        for _ in range(N_RUNS):\n",
    "            gradients, _ = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "            gradients_5_runs.append(gradients)\n",
    "        gradients_mean = np.mean(gradients_5_runs, axis=0)\n",
    "\n",
    "        stddev = np.std(gradients_5_runs, axis=0)\n",
    "        lower = gradients_mean - stddev\n",
    "        upper = gradients_mean + stddev\n",
    "\n",
    "        methods_data[method.name] = {\n",
    "            \"gradient_mean\": list(gradients_mean),\n",
    "            \"lower\": list(lower),\n",
    "            \"upper\": list(upper),\n",
    "            \"n_runs\": N_RUNS,\n",
    "            \"n_iterations\": ITERATIONS\n",
    "        }\n",
    "        datasets_data[dataset_name] = methods_data\n",
    "\n",
    "    break  # plot single dataset\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('run_6_svrg_100_000.json', 'w') as file:\n",
    "    json.dump(datasets_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the mean length of gradients\n",
    "print(\"DONE\")\n",
    "min_length = None\n",
    "for key, val in datasets_data['a1a'].items():\n",
    "    current_length = len(val['gradient_mean'])\n",
    "    if min_length is None:\n",
    "        min_length = current_length\n",
    "    elif min_length > current_length:\n",
    "        min_length = current_length\n",
    "min_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bdef0a40134b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasets_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets_data' is not defined"
     ]
    }
   ],
   "source": [
    "datasets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"run_5.json\", 'r') as file:\n",
    "    datasets_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gradient_mean', 'lower', 'upper', 'n_runs', 'n_iterations'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datasets_data['a1a']['SGD'].keys()\n",
    "# datasets_data['a1a']['AdaSpider']\n",
    "# datasets_data['a1a']['Spider']\n",
    "# datasets_data['a1a']['SpiderBoost']\n",
    "# datasets_data['a1a']['SVRG']\n",
    "# datasets_data['a1a']['AdaGrad']\n",
    "# datasets_data['a1a']['AdaSVRG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_LABEL = \"Stochastic oracle calls\"\n",
    "Y_LABEL = \"||\\u0394f(x)||^2\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5), sharey=False, sharex=False)  # 3, 2\n",
    "\n",
    "for i, dataset_name in enumerate(DATASETS):\n",
    "    print(\"Dataset\", dataset_name)\n",
    "\n",
    "    methods_data = datasets_data[dataset_name]\n",
    "\n",
    "    # sbplt = ax[i%3, i%2]\n",
    "    sbplt = ax\n",
    "\n",
    "    for method_name, method_data in methods_data.items():\n",
    "        print(\"Method\", method_name)\n",
    "        gradients_mean = method_data['gradient_mean']\n",
    "        lower = method_data['lower']\n",
    "        upper = method_data['upper']\n",
    "        n_iterations = method_data['n_iterations']\n",
    "\n",
    "        sbplt.plot(gradients_mean, label=method_name)\n",
    "        sbplt.fill_between(list(range(len(gradients_mean[:min_length]))), lower[:min_length], upper[:min_length], alpha=0.25,\n",
    "                           facecolor='red', edgecolor='red')\n",
    "\n",
    "    sbplt.set_xscale('log')\n",
    "    sbplt.set_yscale('log')\n",
    "    # set y tics\n",
    "    # print(list(np.arange(1e-24))[::-1])\n",
    "    # sbplt.set_ytics(list(np.arange(1e-24))[::-1])\n",
    "    sbplt.set_title(dataset_name)\n",
    "    sbplt.set_xlabel(X_LABEL)\n",
    "    sbplt.set_ylabel(Y_LABEL)\n",
    "    sbplt.legend(loc='lower left', fontsize='small')\n",
    "\n",
    "    break  # plot single dataset\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig(f'tests_logistic_regression_run_6_SVRG.jpg', dpi=300)\n",
    "\n",
    "# plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Do parameter sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "powers = np.array([range(-5, 0)], dtype=float)\n",
    "parameters = 10**powers.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset a1a Iterations:  3000\n",
      "Parameters values:  [1.e-05 1.e-04 1.e-03 1.e-02 1.e-01]\n",
      "Method SGD\n",
      "Method Spider\n",
      "Method SpiderBoost\n",
      "Method SVRG\n",
      "Method AdaGrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ML-Reproducibility-Challange/src/logistic_regression/log_reg.py:41: RuntimeWarning: overflow encountered in exp\n",
      "  first_component = np.log(1 + np.exp(tx.dot(w)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method AdaSVRG\n"
     ]
    }
   ],
   "source": [
    "REDUCTION_STEP = 100\n",
    "\n",
    "METHODS = [\n",
    "    SGD(lambda_=0.01, q=REDUCTION_STEP),\n",
    "    # AdaSpider(q=REDUCTION_STEP),\n",
    "    Spider(n_0 = 1, epsilon=0.01, lambda_=0.01, q=REDUCTION_STEP),\n",
    "    SpiderBoost(lambda_=0.01, q=REDUCTION_STEP),\n",
    "    SVRG(lambda_=0.001, q=REDUCTION_STEP),\n",
    "    AdaGrad(lambda_=0.5, epsilon= 0.00001, q=REDUCTION_STEP),\n",
    "    AdaSVRG(lambda_=0.1, epsilon= 0.00001, q=REDUCTION_STEP)\n",
    "]\n",
    "\n",
    "DF_COLUMNS = ['method_name', 'loss', 'parameters']\n",
    "df = pd.DataFrame(columns=DF_COLUMNS)\n",
    "\n",
    "dataset_name = 'a1a'\n",
    "ITERATIONS = 3000\n",
    "print(\"Dataset\", dataset_name, \"Iterations: \", ITERATIONS)\n",
    "\n",
    "powers = np.array([range(-5, 0)], dtype=float)\n",
    "parameters = 10**powers.flatten()\n",
    "\n",
    "X, y = get_data(dataset_name)\n",
    "tx = build_model(X, y)\n",
    "initial_weights = get_initial_weights(tx)\n",
    "\n",
    "print(\"Parameters values: \", parameters)\n",
    "\n",
    "for method in METHODS:\n",
    "    print(\"Method\", method.name)\n",
    "    if method.n_params_to_tune == 2:\n",
    "        for param in list(product(parameters, parameters)):\n",
    "            method.set_params(param[0], param[1])\n",
    "            _, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "            df = df.append(dict(zip(DF_COLUMNS, [method.name, np.sum(losses), str(param)])), ignore_index=True)\n",
    "    \n",
    "    elif method.n_params_to_tune == 3:\n",
    "        for param in list(product(parameters, parameters)):\n",
    "            method.set_params(param[0], param[1], param[2])\n",
    "            _, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "            df = df.append(dict(zip(DF_COLUMNS, [method.name, np.sum(losses), str(param)])), ignore_index=True)\n",
    "\n",
    "    \n",
    "    elif method.n_params_to_tune == 1:\n",
    "        for param in parameters:\n",
    "            method.set_params(param)\n",
    "            _, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "            df = df.append(dict(zip(DF_COLUMNS, [method.name, np.sum(losses), str(param)])), ignore_index=True)\n",
    "    else:\n",
    "        _, losses = test_method(method, initial_weights, tx, y, ITERATIONS)\n",
    "        df = df.append(dict(zip(DF_COLUMNS, [method.name, np.sum(losses), str(param)])), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"parameters_sweep_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method_name</th>\n",
       "      <th>loss</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD</td>\n",
       "      <td>1855.114364</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD</td>\n",
       "      <td>323.886964</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD</td>\n",
       "      <td>7221.606880</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>78914.015151</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD</td>\n",
       "      <td>612348.603799</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>AdaSVRG</td>\n",
       "      <td>3622.251210</td>\n",
       "      <td>(0.1, 1e-05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>AdaSVRG</td>\n",
       "      <td>3622.496966</td>\n",
       "      <td>(0.1, 0.0001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>AdaSVRG</td>\n",
       "      <td>3622.494324</td>\n",
       "      <td>(0.1, 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>AdaSVRG</td>\n",
       "      <td>3622.497517</td>\n",
       "      <td>(0.1, 0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>AdaSVRG</td>\n",
       "      <td>3622.497195</td>\n",
       "      <td>(0.1, 0.1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   method_name           loss     parameters\n",
       "0          SGD    1855.114364          1e-05\n",
       "1          SGD     323.886964         0.0001\n",
       "2          SGD    7221.606880          0.001\n",
       "3          SGD   78914.015151           0.01\n",
       "4          SGD  612348.603799            0.1\n",
       "..         ...            ...            ...\n",
       "85     AdaSVRG    3622.251210   (0.1, 1e-05)\n",
       "86     AdaSVRG    3622.496966  (0.1, 0.0001)\n",
       "87     AdaSVRG    3622.494324   (0.1, 0.001)\n",
       "88     AdaSVRG    3622.497517    (0.1, 0.01)\n",
       "89     AdaSVRG    3622.497195     (0.1, 0.1)\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaGrad</th>\n",
       "      <td>2059.296693</td>\n",
       "      <td>(0.0001, 0.0001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaSVRG</th>\n",
       "      <td>3622.251210</td>\n",
       "      <td>(0.0001, 0.0001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>323.886964</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVRG</th>\n",
       "      <td>385412.914067</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spider</th>\n",
       "      <td>1543.390569</td>\n",
       "      <td>(0.0001, 0.0001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpiderBoost</th>\n",
       "      <td>1603.184718</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      loss        parameters\n",
       "method_name                                 \n",
       "AdaGrad        2059.296693  (0.0001, 0.0001)\n",
       "AdaSVRG        3622.251210  (0.0001, 0.0001)\n",
       "SGD             323.886964            0.0001\n",
       "SVRG         385412.914067            0.0001\n",
       "Spider         1543.390569  (0.0001, 0.0001)\n",
       "SpiderBoost    1603.184718            0.0001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['loss'] = df['loss'].abs()\n",
    "df.groupby('method_name').min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# METHODS = ['AdaSpider', 'Spider']\n",
    "# ITERATIONS = 100\n",
    "#\n",
    "# def plot_data():\n",
    "#     # Write your code to make 4x4 panel here\n",
    "#     X_LABEL = \"Stochastic oracle calls\"\n",
    "#     Y_LABEL = \"||\\u0394f(x)||^2\"\n",
    "#\n",
    "#     fig, ax = plt.subplots(3,2,figsize=(16,16), sharey=False, sharex=False)\n",
    "#\n",
    "#     for i, dataset_name in enumerate(DATASETS):\n",
    "#         sbplt = ax[i%3, i%2]\n",
    "#         print(dataset_name)\n",
    "#         for method in METHODS:\n",
    "#             if method == \"Spider\":\n",
    "#                 spider_params = SpiderParam(100, 5, 0.05)\n",
    "#                 gradients = test_method(Spider, dataset_name, ITERATIONS, spider_params)\n",
    "#             else:\n",
    "#                 gradients = test_method(ADASpider, dataset_name, ITERATIONS)\n",
    "#             gradients = [np.linalg.norm(grad, 2) for grad in gradients]\n",
    "#             sbplt.plot(gradients, label=method)\n",
    "#\n",
    "#         sbplt.set_xscale('log')\n",
    "#         sbplt.set_title(dataset_name)\n",
    "#         sbplt.set_xlabel(X_LABEL)\n",
    "#         sbplt.set_ylabel(Y_LABEL)\n",
    "#         sbplt.legend(loc='lower left')\n",
    "#\n",
    "#         break  # plot single dataset\n",
    "#\n",
    "#     fig.tight_layout(pad=2.0)\n",
    "#     # fig.savefig('tests_logistic_regression.jpg', dpi=150)\n",
    "#\n",
    "# plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
